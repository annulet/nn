{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ataga\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ataga\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ataga\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ataga\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ataga\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ataga\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the images.\n",
    "x_train = (x_train / 255) - 0.5\n",
    "x_test = (x_test / 255) - 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "x_train = x_train.reshape((-1, 784))\n",
    "x_test = x_test.reshape((-1, 784))\n",
    "\n",
    "print(x_train.shape) # (60000, 784)\n",
    "print(x_test.shape)  # (10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ataga\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ataga\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model.\n",
    "# model = Sequential([\n",
    "#   Dense(128, activation='relu', input_shape=(784,)),\n",
    "#   Dense(128, activation='relu'),\n",
    "#   Dense(10, activation='softmax'),\n",
    "# ])\n",
    "\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "N_CLASSES = 10\n",
    "OPTIMISER = Adam()\n",
    "N_HIDDEN = 256\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN, activation='relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer=OPTIMISER,\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ataga\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.5717 - acc: 0.7928 - val_loss: 0.4152 - val_acc: 0.8484\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.4132 - acc: 0.8490 - val_loss: 0.3725 - val_acc: 0.8619\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.3774 - acc: 0.8608 - val_loss: 0.3552 - val_acc: 0.8719\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.3512 - acc: 0.8713 - val_loss: 0.3419 - val_acc: 0.8753\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.3352 - acc: 0.8761 - val_loss: 0.3375 - val_acc: 0.8792\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.3174 - acc: 0.8823 - val_loss: 0.3398 - val_acc: 0.8762\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.3083 - acc: 0.8863 - val_loss: 0.3355 - val_acc: 0.8762\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.2964 - acc: 0.8888 - val_loss: 0.3385 - val_acc: 0.8802\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.2894 - acc: 0.8914 - val_loss: 0.3317 - val_acc: 0.8809\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.2801 - acc: 0.8947 - val_loss: 0.3204 - val_acc: 0.8879\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.2691 - acc: 0.8989 - val_loss: 0.3180 - val_acc: 0.8862\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.2653 - acc: 0.9019 - val_loss: 0.3208 - val_acc: 0.8873\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.2577 - acc: 0.9043 - val_loss: 0.3250 - val_acc: 0.8839\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.2503 - acc: 0.9058 - val_loss: 0.3084 - val_acc: 0.8881\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2420 - acc: 0.9080 - val_loss: 0.3103 - val_acc: 0.8882\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.2352 - acc: 0.9113 - val_loss: 0.3117 - val_acc: 0.8880\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.2322 - acc: 0.9126 - val_loss: 0.3184 - val_acc: 0.8922\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.2260 - acc: 0.9140 - val_loss: 0.3068 - val_acc: 0.8917\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.2209 - acc: 0.9172 - val_loss: 0.3085 - val_acc: 0.8924\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.2177 - acc: 0.9178 - val_loss: 0.3110 - val_acc: 0.8921\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2107 - acc: 0.9199 - val_loss: 0.3232 - val_acc: 0.8903\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.2060 - acc: 0.9229 - val_loss: 0.3065 - val_acc: 0.8958\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.2036 - acc: 0.9233 - val_loss: 0.3160 - val_acc: 0.8941\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.1984 - acc: 0.9244 - val_loss: 0.3151 - val_acc: 0.8956\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.1939 - acc: 0.9273 - val_loss: 0.3213 - val_acc: 0.8942\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1896 - acc: 0.9273 - val_loss: 0.3244 - val_acc: 0.8940\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.1898 - acc: 0.9282 - val_loss: 0.3101 - val_acc: 0.8953\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.1804 - acc: 0.9312 - val_loss: 0.3228 - val_acc: 0.8947\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1812 - acc: 0.9311 - val_loss: 0.3308 - val_acc: 0.8913\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1809 - acc: 0.9316 - val_loss: 0.3134 - val_acc: 0.8961\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1733 - acc: 0.9330 - val_loss: 0.3285 - val_acc: 0.8958\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1681 - acc: 0.9370 - val_loss: 0.3297 - val_acc: 0.8942\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1668 - acc: 0.9346 - val_loss: 0.3345 - val_acc: 0.8947\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.1613 - acc: 0.9376 - val_loss: 0.3387 - val_acc: 0.8951\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.1647 - acc: 0.9374 - val_loss: 0.3327 - val_acc: 0.8977\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1577 - acc: 0.9397 - val_loss: 0.3485 - val_acc: 0.8915\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.1575 - acc: 0.9407 - val_loss: 0.3300 - val_acc: 0.8925\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.1554 - acc: 0.9418 - val_loss: 0.3323 - val_acc: 0.8975\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.1522 - acc: 0.9417 - val_loss: 0.3414 - val_acc: 0.8962\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.1485 - acc: 0.9438 - val_loss: 0.3492 - val_acc: 0.8962\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1461 - acc: 0.9434 - val_loss: 0.3579 - val_acc: 0.8957\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1492 - acc: 0.9431 - val_loss: 0.3513 - val_acc: 0.8920\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1449 - acc: 0.9448 - val_loss: 0.3667 - val_acc: 0.8933\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1401 - acc: 0.9465 - val_loss: 0.3507 - val_acc: 0.8969\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1410 - acc: 0.9468 - val_loss: 0.3522 - val_acc: 0.8966\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.1380 - acc: 0.9473 - val_loss: 0.3461 - val_acc: 0.8980\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.1345 - acc: 0.9498 - val_loss: 0.3686 - val_acc: 0.8962\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1358 - acc: 0.9479 - val_loss: 0.3672 - val_acc: 0.8925\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1331 - acc: 0.9485 - val_loss: 0.3654 - val_acc: 0.8936\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1242 - acc: 0.9514 - val_loss: 0.3724 - val_acc: 0.8972\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1270 - acc: 0.9512 - val_loss: 0.3718 - val_acc: 0.8981\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1290 - acc: 0.9521 - val_loss: 0.3666 - val_acc: 0.8982\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1268 - acc: 0.9517 - val_loss: 0.3731 - val_acc: 0.8932\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1236 - acc: 0.9531 - val_loss: 0.3682 - val_acc: 0.8995\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1227 - acc: 0.9529 - val_loss: 0.3840 - val_acc: 0.8951\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1179 - acc: 0.9544 - val_loss: 0.3770 - val_acc: 0.8953\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1159 - acc: 0.9555 - val_loss: 0.3739 - val_acc: 0.8963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1180 - acc: 0.9551 - val_loss: 0.3809 - val_acc: 0.9008\n",
      "Epoch 59/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1181 - acc: 0.9567 - val_loss: 0.3789 - val_acc: 0.8982\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1161 - acc: 0.9560 - val_loss: 0.3942 - val_acc: 0.8955\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1127 - acc: 0.9572 - val_loss: 0.3925 - val_acc: 0.8936\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1104 - acc: 0.9576 - val_loss: 0.4010 - val_acc: 0.8964\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1143 - acc: 0.9553 - val_loss: 0.3871 - val_acc: 0.8961\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1085 - acc: 0.9588 - val_loss: 0.3837 - val_acc: 0.8990\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1057 - acc: 0.9608 - val_loss: 0.3978 - val_acc: 0.8939\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1153 - acc: 0.9571 - val_loss: 0.3745 - val_acc: 0.8992\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1087 - acc: 0.9584 - val_loss: 0.4073 - val_acc: 0.8958\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1088 - acc: 0.9593 - val_loss: 0.3851 - val_acc: 0.8953\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1044 - acc: 0.9605 - val_loss: 0.4014 - val_acc: 0.8960\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1054 - acc: 0.9604 - val_loss: 0.3874 - val_acc: 0.8973\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0999 - acc: 0.9618 - val_loss: 0.3875 - val_acc: 0.8972\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1055 - acc: 0.9601 - val_loss: 0.4081 - val_acc: 0.8957\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0985 - acc: 0.9631 - val_loss: 0.4113 - val_acc: 0.8949\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1047 - acc: 0.9610 - val_loss: 0.3993 - val_acc: 0.8984\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0989 - acc: 0.9627 - val_loss: 0.4052 - val_acc: 0.8977\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0989 - acc: 0.9621 - val_loss: 0.4104 - val_acc: 0.8950\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1020 - acc: 0.9610 - val_loss: 0.4006 - val_acc: 0.8963\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0989 - acc: 0.9629 - val_loss: 0.4043 - val_acc: 0.8959\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0943 - acc: 0.9643 - val_loss: 0.4187 - val_acc: 0.8959\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0950 - acc: 0.9642 - val_loss: 0.4314 - val_acc: 0.8937\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0960 - acc: 0.9641 - val_loss: 0.4276 - val_acc: 0.8957\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0959 - acc: 0.9639 - val_loss: 0.4139 - val_acc: 0.8948\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.0966 - acc: 0.9634 - val_loss: 0.4070 - val_acc: 0.8949\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0914 - acc: 0.9669 - val_loss: 0.4387 - val_acc: 0.8967\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.0941 - acc: 0.9641 - val_loss: 0.4165 - val_acc: 0.8952\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.0933 - acc: 0.9650 - val_loss: 0.4187 - val_acc: 0.8963\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.0883 - acc: 0.9667 - val_loss: 0.4365 - val_acc: 0.8962\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.0944 - acc: 0.9647 - val_loss: 0.4499 - val_acc: 0.8922\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0871 - acc: 0.9672 - val_loss: 0.4285 - val_acc: 0.8974\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0915 - acc: 0.9670 - val_loss: 0.4270 - val_acc: 0.8962\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0828 - acc: 0.9679 - val_loss: 0.4350 - val_acc: 0.8984\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0903 - acc: 0.9668 - val_loss: 0.4323 - val_acc: 0.8968\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0889 - acc: 0.9676 - val_loss: 0.4508 - val_acc: 0.8921\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0879 - acc: 0.9663 - val_loss: 0.4342 - val_acc: 0.9006\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0851 - acc: 0.9684 - val_loss: 0.4270 - val_acc: 0.8957\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0835 - acc: 0.9690 - val_loss: 0.4376 - val_acc: 0.8976\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0828 - acc: 0.9697 - val_loss: 0.4466 - val_acc: 0.8953\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0826 - acc: 0.9690 - val_loss: 0.4376 - val_acc: 0.8981\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0824 - acc: 0.9691 - val_loss: 0.4410 - val_acc: 0.8958\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0839 - acc: 0.9686 - val_loss: 0.4439 - val_acc: 0.8928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x222cec1bbc8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "model.fit(\n",
    "    x_train,\n",
    "    to_categorical(y_train),\n",
    "    epochs=N_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 18us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47610057914853093, 0.891]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  x_test,\n",
    "  to_categorical(y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('nn_model_hw_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk later using:\n",
    "# model.load_weights('nn_model_hw_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.891"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я увеличила число входных неронов и нейронов скрытого слоя до 256, добавила прореживание при помощи метода Dropout, увеличила размер батча до 128, при обучении использовала валидацию на отложенной выборке, и в итоге получиал 89% точность на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
